* codebase
** TODO Switch these data filters to _experiment_ level inputs: xtype, ytype, filters
** TODO add testing for all model training and eval functions

* dataset
** TODO Try last X/last Y/delta-X (instead of new-X) to make it easier for the random forest -KJ
** TODO Add synthetic dataset to validate model behaviors (any Markov process + noise) -KJ
** DONE for small train sizes, multiple samples from random indices in the run
CLOSED: [2022-08-09 Tue 23:02]
** DONE double check area units and high MAE values
CLOSED: [2022-08-09 Tue 15:53]
** DONE t-1 return type that includes y-spec
CLOSED: [2022-07-26 Tue 17:23]


* models
** TODO Add another baseline - always predict last Y -KJ
** TODO Try cold-start linear last-X/Y (and linear models in general) -KJ
** convolutional input layer transfer?
** latent space inputs and runtime L-estimator
** supervise updating L
** recurrent L estimator
** DONE bootstrap model ensemble and runtime selector
CLOSED: [2022-07-26 Tue 17:24]


* analysis
** TODO Switch nomenclature to "tasks" and "target task" -KJ
** Switch from raw values to +/- dataset mean
Larger data numbers could look better if the larger datasets happen to be easier to predict. Instead we could look at average deviation from the mean for each dataset? Just a thought.
