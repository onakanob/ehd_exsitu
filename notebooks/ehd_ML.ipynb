{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971e0bed-b911-4643-8d7d-0bc136550c44",
   "metadata": {},
   "source": [
    "# Exploring multiple approaches to machine learning across multiple small EHD experimental datasets\n",
    " - The task is to predict (regress) the size of printed features, as a function of waveform inputs to the EHD printer\n",
    " - A second task is to predict (classify) whether a given waveform input will produce any printed pattern at all. (This is equivalent to understanding the print onset voltage threshold.)\n",
    " - Multiple hidden confounding variables are likely, such as ink/tip/substrate/atmospheric condition; ink and tip clogging; print tip height from the substrate. Some of these will be relatively constant for each run of experiments, some will vary from feature to feature.\n",
    " - Ink dynamics at the printing tip are complex and nonlinear, with electrical and fluid/acoustic phenomena that interact with each other.\n",
    "\n",
    "## Goals\n",
    " - In regression, aim for <3% mean absolute error (from the predicted outcome)\n",
    " - In classification, aim for >0.9 ROC AUC\n",
    " - For a new/test set, achieve these in <=100 experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30913236-8a5d-4983-b31b-9ae56f38dbca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from ehd_dataset import EHD_Loader\n",
    "from ehd_models import EHD_Model\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "Dataset_Pkl = \"C:/Dropbox/SPEED/Self Driving EHD/Datasets/compiled_data.pickle\"\n",
    "\n",
    "# Load up the dataset\n",
    "loader = EHD_Loader(Dataset_Pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5afbe9d-9041-40c6-9389-633bf5cb5eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating regression model type: scaling_MLP\n",
      "fold 0 pretrain... done fold 1 pretrain... done fold 2 pretrain... done fold 3 pretrain... done "
     ]
    }
   ],
   "source": [
    "# REGRESSION MODELS\n",
    "from ehd_models import EHD_Model\n",
    "\n",
    "# Print Area Regression Models <<<<<<<<<<<<<<<<<<<<\n",
    "REGRESSION_model_architectures = {\n",
    "                                  # 'MLE': {},\n",
    "                                  # 'cold_RF': {},\n",
    "                                  # # 'only_pretrained_RF': {},  # RF_Regressor_Allpre\n",
    "                                  # 'normed_RF': {\n",
    "                                  #     'bootstrap': True,\n",
    "                                  #     'max_depth': 49,\n",
    "                                  #     'max_leaf_nodes': 42,\n",
    "                                  #     'max_samples': 13,\n",
    "                                  #     'min_samples_leaf': 1,\n",
    "                                  #     'min_samples_split': 4,\n",
    "                                  #     'n_estimators': 434,\n",
    "                                  # },  # RF_Regressor_Allpre; xtype=normed_squares\n",
    "                                  # 'normed_MLP': {\n",
    "                                  #     'activation': 'relu',\n",
    "                                  #     'alpha': \t4.105E-05,\n",
    "                                  #     'batch_size': 184,\n",
    "                                  #     'beta_1': 0.070320529,\n",
    "                                  #     'beta_2': 0.000569048,\n",
    "                                  #     'layer1_size': 36,\n",
    "                                  #     'layer2_size': 36,\n",
    "                                  #     'lr': 1.96866E-05,\n",
    "                                  # },  # MLP_Regressor_Allpre; xtype=normed_squares\n",
    "                                  # 'warm_MLP': {\n",
    "                                  #     'activation': 'tanh',\n",
    "                                  #     'alpha': 8.35434E-06,\n",
    "                                  #     'batch_size': 143,\n",
    "                                  #     'beta_1': 0.199856952,\n",
    "                                  #     'beta_2': 0.000731856,\n",
    "                                  #     'layer1_size': 152,\n",
    "                                  #     'layer2_size': 51,\n",
    "                                  #     'lr': 1.54152E-05,\n",
    "                                  #     'retrain_alpha': 8.85273E-05,\n",
    "                                  #     'retrain_lr': 0.006031221,\n",
    "                                  # },\n",
    "                                  # 'warm_RF': {\n",
    "                                  #     'bootstrap': False,\n",
    "                                  #     'max_depth': 87,\n",
    "                                  #     'max_leaf_nodes': 43,\n",
    "                                  #     'max_samples': 32,\n",
    "                                  #     'min_samples_leaf': 1,\n",
    "                                  #     'min_samples_split': 4,\n",
    "                                  #     'n_estimators': 25,\n",
    "                                  #     'new_estimators': 125,\n",
    "                                  # },\n",
    "                                  # 'reweight_RF_wide': {\n",
    "                                  #     'bootstrap': True,\n",
    "                                  #     'max_depth': 1,\n",
    "                                  #     'max_leaf_nodes': 7,\n",
    "                                  #     'max_samples': 23,\n",
    "                                  #     'min_samples_leaf': 1,\n",
    "                                  #     'min_samples_split': 7,\n",
    "                                  #     'n_estimators': 616,\n",
    "                                  #     'alpha': 0.048167146,\n",
    "                                  # },\n",
    "                                  # 'reweight_RF_tall': {\n",
    "                                  #     'bootstrap': True,\n",
    "                                  #     'max_depth': 68,\n",
    "                                  #     'max_leaf_nodes': 78,\n",
    "                                  #     'max_samples': 14,\n",
    "                                  #     'min_samples_leaf': 1,\n",
    "                                  #     'min_samples_split': 5,\n",
    "                                  #     'n_estimators': 49,\n",
    "                                  #     'alpha': 0.00290017,\n",
    "                                  # },\n",
    "                                  'scaling_MLP': {\n",
    "                                      'activation': 'relu',\n",
    "                                      'alpha': 1.55161E-05,\n",
    "                                      'batch_size': 24,\n",
    "                                      'beta_1': 0.059863626,\n",
    "                                      'beta_2': 0.000977064,\n",
    "                                      'final_alpha': 8.545432376,\n",
    "                                      'layer1_size': 44,\n",
    "                                      'layer2_size': 253,\n",
    "                                      'lr': 2.07268E-05,\n",
    "                                  },\n",
    "                                  # 'v_normed_RF': {},  # RF_Regressor_Allpre; xtype=v_normed_squares\n",
    "                                  # 'v_normed_Ridge': {},  # Ridge_Regressor_Allpre; xtype=v_normed_squares\n",
    "                                  # 'v_normed_MLP': {},  # MLP_Regressor_Allpre; xtype=v_normed_squares\n",
    "                                  # 'cold_MLP': {'max_iter': 100_000},\n",
    "                                  # 'lastXY_MLP': {'max_iter': 200_000},\n",
    "                                  # 'lastXY_RF': {}\n",
    "                                 }\n",
    "NEW_CACHE = False\n",
    "OVERWRITE_RESULTS = True\n",
    "\n",
    "XTYPE = \"normed_squares\"  # vector, normed_squares, v_normed_squares\n",
    "YTYPE = \"max_width\"\n",
    "FILTERS = [\n",
    "           ('vector',  lambda x: len(x), 2),\n",
    "           ('Wavegen', lambda x: x, 'square'),\n",
    "           ('V Thresh [V] @ .5s', np.isnan, False),\n",
    "           ('SIJ Tip', lambda x: x, 'Std'),\n",
    "           ('jetted',  lambda x: x, True),\n",
    "           ('clogging', lambda x: x, False)\n",
    "          ]\n",
    "\n",
    "CACHE_NAME = 'regression.cache'\n",
    "\n",
    "if NEW_CACHE:\n",
    "    reg_results = []\n",
    "    reg_names = []\n",
    "else:\n",
    "    with open(CACHE_NAME, 'rb') as f:\n",
    "        (reg_results, reg_names) = pickle.load(f)\n",
    "\n",
    "for architecture, params in REGRESSION_model_architectures.items():\n",
    "    if (architecture in reg_names) and not OVERWRITE_RESULTS:\n",
    "        print(f'{architecture} was already evaluated - loaded from cache')\n",
    "    else:\n",
    "        print('')\n",
    "        if architecture in reg_names:\n",
    "            print(f\"Overwriting previous results from {architecture}\")\n",
    "            reg_results = [reg_results[i] for i in range(len(reg_results)) if reg_names[i] != architecture]\n",
    "            reg_names = [r for r in reg_names if r != architecture]\n",
    "        print(f'Evaluating regression model type: {architecture}')\n",
    "        model = EHD_Model(architecture=architecture, params=params)\n",
    "\n",
    "        # for dataset_fold in range(loader.num_folds()):\n",
    "        for dataset_fold in range(loader.num_folds(FILTERS)):\n",
    "            print(f\"fold {dataset_fold}\", end=\" \")\n",
    "            pretrain_set, eval_set, eval_name = loader.folded_dataset(fold=dataset_fold, xtype=XTYPE, ytype=YTYPE,\n",
    "                                                                     pretrain=model.pretrainer, filters=FILTERS)\n",
    "            # try:\n",
    "            if model.pretrainer:\n",
    "                print('pretrain...', end=' ')\n",
    "                model.pretrain(pretrain_set)\n",
    "                print('done', end=' ')\n",
    "\n",
    "            output = model.evaluate(eval_set)\n",
    "\n",
    "            output['architecture'] = architecture\n",
    "            output['eval_dataset'] = eval_name\n",
    "            reg_names.append(architecture)\n",
    "            reg_results.append(output)\n",
    "\n",
    "with open(CACHE_NAME, 'wb') as f:\n",
    "    pickle.dump((reg_results, reg_names), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97aae51-86f4-4243-9e8f-e7536c103933",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting previous results from MLE_class\n",
      "\n",
      "Evaluating classification model type: MLE_class\n",
      "fold 0 fold 1 fold 2 fold 3 Overwriting previous results from cold_RF_class\n",
      "\n",
      "Evaluating classification model type: cold_RF_class\n",
      "fold 0 fold 1 fold 2 fold 3 Overwriting previous results from normed_RF_class\n",
      "\n",
      "Evaluating classification model type: normed_RF_class\n",
      "fold 0 fold 1 fold 2 fold 3 Overwriting previous results from normed_MLP_class\n",
      "\n",
      "Evaluating classification model type: normed_MLP_class\n",
      "fold 0 fold 1 fold 2 fold 3 "
     ]
    }
   ],
   "source": [
    "# JETTING CLASSIFICATION MODELS\n",
    "from ehd_models import EHD_Model\n",
    "\n",
    "# Jetting Classification Models <<<<<<<<<<<<<<<<<<<<\n",
    "CLASS_model_architectures = {\n",
    "                             'MLE_class': {},\n",
    "                             'cold_RF_class': {},\n",
    "                             # 'only_pretrained_RF_class': {},  # RF_Classifier_Allpre\n",
    "                             'normed_RF_class': {},  # RF_Classifier_Allpre; xtype=normed_squares\n",
    "                             'normed_MLP_class': {\n",
    "                                 'activation': 'tanh',\n",
    "                                 'alpha': 5.98012E-05,\n",
    "                                 'batch_size': 145,\n",
    "                                 'beta_1': 0.075248788,\n",
    "                                 'beta_2': 0.001263418,\n",
    "                                 'layer1_size': 78,\n",
    "                                 'layer2_size': 298,\n",
    "                                 'lr': 4.97715E-05,\n",
    "                             },  # RF_Classifier_Allpre; xtype=normed_squares\n",
    "                             # 'v_normed_RF_class': {},  # RF_Classifier_Allpre; xtype=v_normed_squares\n",
    "                             # 'cold_MLP_class': {'max_iter': 100_000},\n",
    "                             # 'lastXY_MLP_class': {'max_iter': 200_000},\n",
    "                             # 'lastXY_RF_class': {}\n",
    "                            }\n",
    "NEW_CACHE = False\n",
    "OVERWRITE_RESULTS = False\n",
    "\n",
    "XTYPE = \"normed_squares\"  # vector, normed_squares, v_normed_squares\n",
    "YTYPE = \"jetted\"\n",
    "FILTERS = [\n",
    "           ('vector', lambda x: len(x), 2),\n",
    "           ('Wavegen', lambda x: x, 'square'),\n",
    "           ('V Thresh [V] @ .5s', np.isnan, False),\n",
    "           ('SIJ Tip', lambda x: x, 'Std'),\n",
    "           ('clogging', lambda x: x, False)\n",
    "          ]\n",
    "\n",
    "CACHE_NAME = 'classification.cache'\n",
    "\n",
    "if NEW_CACHE:\n",
    "    class_results = []\n",
    "    class_names = []\n",
    "else:\n",
    "    with open(CACHE_NAME, 'rb') as f:\n",
    "        (class_results, class_names) = pickle.load(f)\n",
    "    \n",
    "for architecture, params in CLASS_model_architectures.items():\n",
    "    if (architecture in class_names) and not OVERWRITE_RESULTS:\n",
    "        print(f'{architecture} was already evaluated - loaded from cache')\n",
    "    else:\n",
    "        if architecture in class_names:\n",
    "            print(f\"Overwriting previous results from {architecture}\")\n",
    "            class_results = [class_results[i] for i in range(len(class_results)) if class_names[i] != architecture]\n",
    "            class_names = [c for c in class_names if c != architecture]\n",
    "        print(f'\\nEvaluating classification model type: {architecture}')\n",
    "        model = EHD_Model(architecture=architecture, params=params)\n",
    "\n",
    "        for dataset_fold in range(loader.num_folds(FILTERS)):\n",
    "            print(f\"fold {dataset_fold}\", end=\" \")\n",
    "            pretrain_set, eval_set, eval_name = loader.folded_dataset(fold=dataset_fold, xtype=XTYPE, ytype=YTYPE,\n",
    "                                                                     pretrain=model.pretrainer, filters=FILTERS)\n",
    "            model.pretrain(pretrain_set)\n",
    "            output = model.evaluate(eval_set)\n",
    "\n",
    "            output['architecture'] = architecture\n",
    "            output['eval_dataset'] = eval_name\n",
    "            class_names.append(architecture)\n",
    "            class_results.append(output)\n",
    "\n",
    "with open(CACHE_NAME, 'wb') as f:\n",
    "    pickle.dump((class_results, class_names), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4321b309-62d2-4949-80b3-e819ea6e1188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression architectures: ['MLE' 'cold_RF' 'normed_RF' 'normed_MLP' 'warm_MLP' 'warm_RF'\n",
      " 'reweight_RF_wide' 'reweight_RF_tall' 'scaling_MLP']\n",
      "Classification architectures: ['MLE_class' 'cold_RF_class' 'normed_RF_class' 'normed_MLP_class']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE FIG and SUMMARY FILES\n",
    "import seaborn as sns\n",
    "\n",
    "OUT_XLSX = 'eval_summary.xlsx'\n",
    "xlsx_writer = pd.ExcelWriter(OUT_XLSX)\n",
    "plt.rcParams['figure.figsize'] = (11.0, 10.0) # set default size of plots\n",
    "plt.clf()\n",
    "\n",
    "def n_walk_result_vis(df, x, y, trends, ax=None, outfile=None, legend='auto', labels=None):\n",
    "    bar_ci = 0.95  # 'sd' for standard deviation\n",
    "    sns.lineplot(\n",
    "        data=df.loc[~df['unique N']],\n",
    "        x=x, y=y, hue=trends, style=trends,\n",
    "        markers=True,\n",
    "        err_style=\"bars\", ci=bar_ci,\n",
    "        markersize=7,\n",
    "        linewidth=1.8,\n",
    "        ax=ax,\n",
    "        legend=legend\n",
    "    )\n",
    "    ticks = df[x].loc[~df['unique N']].unique()\n",
    "    ax.set_xticks(ticks=ticks,\n",
    "               labels=np.round(10**ticks).astype(int))\n",
    "    ax.set_xlabel('dataset size')\n",
    "    if legend is not None and labels is not None:\n",
    "        ax.legend(labels)\n",
    "    if y == 'MSE':\n",
    "        ax.set_ylim(0, 16e3)\n",
    "    if y == 'MAE':\n",
    "        ax.set_ylim(0, 40)\n",
    "    if y == 'r':\n",
    "        ax.set_ylim(0, 1)\n",
    "    if y == 'F1':\n",
    "        ax.set_ylim(0.49, 1)\n",
    "    if y == 'AUC':\n",
    "        ax.set_ylim(0.49, 1)\n",
    "    if y == 'MAPE':\n",
    "        ax.set_ylim(0, 100)\n",
    "    if outfile is not None:\n",
    "        plt.savefig(outfile, bbox_inches='tight', pad_inches=0.1, transparent=False, dpi=500)\n",
    "        plt.clf()\n",
    "\n",
    "# REGRESSION: Expand, log, and visualize results\n",
    "df = pd.concat(reg_results, ignore_index=True)\n",
    "# labels = ('MLE', 'Cold Random Forest', 'Pretrained Random Forest', 'normed MLE', 'normed RF', 'voltage-norm RF')\n",
    "labels = ('MLE', 'cold-start Random Forest', 'zero-shot Random Forest', 'zero-shot MLP', 'warm-start MLP', 'warm-start Random Forest', 'reweighed RF (wide)', 'reweighed RF (tall)', 'post-scaling MLP')\n",
    "print(f\"Regression architectures: {df.architecture.unique()}\")\n",
    "df['log_size'] = df['train_size'].apply(lambda x: np.log10(x).round(3))\n",
    "df['MAPE'] = df['MAPE'].apply(lambda x: 100 * x)\n",
    "df['unique N'] = df['train_size'].apply(lambda x: (df['train_size'] == x).sum() <= len(df['architecture'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "legend = 'auto'\n",
    "for i, metric in enumerate(['MAE', 'r']):\n",
    "    n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', ax=ax[0][i], legend=legend, labels=labels)\n",
    "    labels = None\n",
    "    legend = False\n",
    "    \n",
    "df.to_excel(xlsx_writer, index=False, sheet_name='Regression')\n",
    "\n",
    "\n",
    "# CLASSIFICATION: Expand, log, and visualize results\n",
    "# labels = ('MLE', 'Cold Random Forest', 'Pretrained Random Forest', 'normed MLE', 'normed RF', 'voltage-norm RF')\n",
    "labels = ('MLE', 'cold start RF', 'zero-shot RF', 'zero-shot MLP')\n",
    "df = pd.concat(class_results, ignore_index=True)\n",
    "print(f\"Classification architectures: {df.architecture.unique()}\")\n",
    "df['log_size'] = df['train_size'].apply(lambda x: np.log10(x).round(3))\n",
    "df['unique N'] = df['train_size'].apply(lambda x: (df['train_size'] == x).sum() <= len(df['architecture'].unique()))\n",
    "    \n",
    "df.to_excel(xlsx_writer, index=False, sheet_name='Classification')\n",
    "\n",
    "legend = 'auto'\n",
    "for i, metric in enumerate(['AUC', 'F1']):\n",
    "    n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', ax=ax[1][i], legend=legend, labels=labels)\n",
    "    labels = None\n",
    "    legend = False\n",
    "\n",
    "\n",
    "plt.savefig('4-up.png', bbox_inches='tight', pad_inches=0.1, transparent=False, dpi=300)\n",
    "plt.clf()\n",
    "xlsx_writer.save()\n",
    "xlsx_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
