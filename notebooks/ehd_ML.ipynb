{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971e0bed-b911-4643-8d7d-0bc136550c44",
   "metadata": {},
   "source": [
    "# Exploring multiple approaches to machine learning across multiple small EHD experimental datasets\n",
    " - The task is to predict (regress) the size of printed features, as a function of waveform inputs to the EHD printer\n",
    " - A second task is to predict (classify) whether a given waveform input will produce any printed pattern at all. (This is equivalent to understanding the print onset voltage threshold.)\n",
    " - Multiple hidden confounding variables are likely, such as ink/tip/substrate/atmospheric condition; ink and tip clogging; print tip height from the substrate. Some of these will be relatively constant for each run of experiments, some will vary from feature to feature.\n",
    " - Ink dynamics at the printing tip are complex and nonlinear, with electrical and fluid/acoustic phenomena that interact with each other.\n",
    "\n",
    "## Goals\n",
    " - In regression, aim for <3% mean absolute error (from the predicted outcome)\n",
    " - In classification, aim for >0.9 ROC AUC\n",
    " - For a new/test set, achieve these in <=100 experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30913236-8a5d-4983-b31b-9ae56f38dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "sys.path.append('..')\n",
    "from ehd_dataset import EHD_Loader\n",
    "from ehd_models import EHD_Model\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "NEW_CACHE = True\n",
    "# TODO option to bypass the cache op for nondestructive testing\n",
    "\n",
    "# Print Area Regression Models <<<<<<<<<<<<<<<<<<<<\n",
    "REGRESSION_model_architectures = {'MLE': {},\n",
    "                                  # 'cold_MLP': {'max_iter': 100_000},\n",
    "                                  # 'lastXY_MLP': {'max_iter': 200_000},\n",
    "                                  'all_pretrained_RF': {},\n",
    "                                  # 'lastXY_RF': {},\n",
    "                                  'cold_RF': {}}\n",
    "\n",
    "# Jetting Classification Models <<<<<<<<<<<<<<<<<<<<\n",
    "CLASS_model_architectures = {'MLE_class': {},\n",
    "                             # 'cold_MLP_class': {'max_iter': 100_000},\n",
    "                             # 'lastXY_MLP_class': {'max_iter': 200_000},\n",
    "                             'all_pretrained_RF_class': {},\n",
    "                             # 'lastXY_RF_class': {},\n",
    "                             'cold_RF_class': {}}\n",
    "\n",
    "XTYPE = \"square\"\n",
    "YTYPE = \"diameter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c326ac0-8e77-4024-8fde-d3bfe5aaa08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load 10-Mar-2022 large nozzle mosaic: 'DataFrame' object has no attribute 'note'\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\29-Mar-2022 lg 1cm 300 points\t263 points\toffset 2\tcorr 0.4979414348873561\n",
      "> \u001b[1;32mc:\\dropbox\\python\\ehd_exsitu\\ehd_dataset\\dataset.py\u001b[0m(111)\u001b[0;36m__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    110 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 111 \u001b[1;33m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    112 \u001b[1;33m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  df.head()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   obj_count           area  print_length   max_width  mean_width  \\\n",
      "0          2   17899.985347    201.025641   28.717949   21.161695   \n",
      "1          0       0.000000      0.000000    0.000000    0.000000   \n",
      "2          1  138870.954565    281.823136  153.094451  117.107313   \n",
      "3          1   27257.198260    225.710919   38.605398   28.699735   \n",
      "4          6     371.809785    127.179487    6.153846    0.694789   \n",
      "\n",
      "                                                wave  \\\n",
      "0  [0.0, -16.7835354, -33.49617624, -50.067351599...   \n",
      "1  [0.0, 7.9345913999999995, 15.83229828, 23.6564...   \n",
      "2  [0.0, -15.240176279999998, -30.41114928, -45.4...   \n",
      "3  [0.0, 14.27829036, 28.480577519999997, 42.5312...   \n",
      "4  [0.0, 7.17114204, 14.34002904, 21.50435028, 28...   \n",
      "\n",
      "                                              vector  volts  jetted SIJ Tip  \\\n",
      "0  [-211.46407223999998, 234.89078844000002, -167...  348.0    True   Large   \n",
      "1  [125.14207019999999, 86.15063291999999, -87.69...  348.0   False   Large   \n",
      "2  [-165.51479952, -185.0214906, -124.9530018, 33...  348.0    True   Large   \n",
      "3  [-159.9770616, 86.93579052, -178.340952, 8.276...  348.0    True   Large   \n",
      "4  [132.95600304, 108.01834392, 105.1615458, 339....  348.0    True   Large   \n",
      "\n",
      "   Standoff [um]    Wavegen  V Thresh [V] @ .5s  W thresh [s] @ 1.5 Vt  \n",
      "0           20.0  harmonics                 NaN                    NaN  \n",
      "1           20.0  harmonics                 NaN                    NaN  \n",
      "2           20.0  harmonics                 NaN                    NaN  \n",
      "3           20.0  harmonics                 NaN                    NaN  \n",
      "4           20.0  harmonics                 NaN                    NaN  \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  row\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path                                     29-Mar-2022 lg 1cm 300 points\n",
      "Date                                               2022-03-29 00:00:00\n",
      "SIJ Tip                                                          Large\n",
      "Standoff [um]                                                     20.0\n",
      "Velocity [um/s]                                                  100.0\n",
      "Wavegen                                                      harmonics\n",
      "Samples                                                          263.0\n",
      "Hz                                                                 4.0\n",
      "V Thresh [V] @ .5s                                                 NaN\n",
      "W thresh [s] @ 1.5 Vt                                              NaN\n",
      "V at 0.7 scale (go a little lower)                                 NaN\n",
      "width @ 4 Hz (little lower to higher)                              NaN\n",
      "Starting V                                                         NaN\n",
      "Final V                                                            NaN\n",
      "Name: 1, dtype: object\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load 29-Mar-2022 lg 1cm 300 points: \n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\2-May-2022__run 1\t121 points\toffset 32\tcorr 0.6417618745477631\n",
      "> \u001b[1;32mc:\\dropbox\\python\\ehd_exsitu\\ehd_dataset\\dataset.py\u001b[0m(111)\u001b[0;36m__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    110 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 111 \u001b[1;33m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    112 \u001b[1;33m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "INDEX = \"C:/Dropbox/SPEED/Self Driving EHD/Datasets/dataset_index.xlsx\"\n",
    "# INDEX = \"C:/Users/Oliver/Dropbox/SPEED/Self Driving EHD/Datasets/dataset_index.xlsx\"\n",
    "loader = EHD_Loader(INDEX)\n",
    "print(\"Datasets Loaded!\\n>> Quick correlation validation check -- [auac; vec L2] <<\")\n",
    "\n",
    "for i, df in enumerate(loader.datasets):\n",
    "    AUAC, _ = pearsonr(df.area,\n",
    "                       df.wave.apply(lambda x: np.sum(np.abs(x))))\n",
    "    VL2, _ = pearsonr(df.area,\n",
    "                       df.vector.apply(lambda x: np.sqrt(np.sum(x**2))))\n",
    "    print(f\"<<{loader.names[i]} -- [{AUAC:.3f}; {VL2:.3f}]\", end='>> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3430da27-469b-4ac0-9d66-04383e83effe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   obj_count           area  print_length   max_width  mean_width  \\\n",
      "0          2   17899.985347    201.025641   28.717949   21.161695   \n",
      "1          0       0.000000      0.000000    0.000000    0.000000   \n",
      "2          1  138870.954565    281.823136  153.094451  117.107313   \n",
      "3          1   27257.198260    225.710919   38.605398   28.699735   \n",
      "4          6     371.809785    127.179487    6.153846    0.694789   \n",
      "\n",
      "                                                wave  \\\n",
      "0  [0.0, -16.7835354, -33.49617624, -50.067351599...   \n",
      "1  [0.0, 7.9345913999999995, 15.83229828, 23.6564...   \n",
      "2  [0.0, -15.240176279999998, -30.41114928, -45.4...   \n",
      "3  [0.0, 14.27829036, 28.480577519999997, 42.5312...   \n",
      "4  [0.0, 7.17114204, 14.34002904, 21.50435028, 28...   \n",
      "\n",
      "                                              vector  volts  jetted  \n",
      "0  [-211.46407223999998, 234.89078844000002, -167...  348.0    True  \n",
      "1  [125.14207019999999, 86.15063291999999, -87.69...  348.0   False  \n",
      "2  [-165.51479952, -185.0214906, -124.9530018, 33...  348.0    True  \n",
      "3  [-159.9770616, 86.93579052, -178.340952, 8.276...  348.0    True  \n",
      "4  [132.95600304, 108.01834392, 105.1615458, 339....  348.0    True  \n"
     ]
    }
   ],
   "source": [
    "model = EHD_Model(architecture='all_pretrained_RF', params={})\n",
    "print(loader.datasets[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5afbe9d-9041-40c6-9389-633bf5cb5eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'regression.cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     reg_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCACHE_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     10\u001b[0m         (reg_results, reg_names) \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m architecture, params \u001b[38;5;129;01min\u001b[39;00m REGRESSION_model_architectures\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'regression.cache'"
     ]
    }
   ],
   "source": [
    "from ehd_models import EHD_Model\n",
    "\n",
    "CACHE_NAME = 'regression.cache'\n",
    "\n",
    "if NEW_CACHE:\n",
    "    reg_results = []\n",
    "    reg_names = []\n",
    "else:\n",
    "    with open(CACHE_NAME, 'rb') as f:\n",
    "        (reg_results, reg_names) = pickle.load(f)\n",
    "\n",
    "for architecture, params in REGRESSION_model_architectures.items():\n",
    "    if architecture in reg_names:\n",
    "        print(f'{architecture} was already evaluated - loaded from cache')\n",
    "    else:\n",
    "        print(f'\\nEvaluating regression model type: {architecture}')\n",
    "        model = EHD_Model(architecture=architecture, params=params)\n",
    "\n",
    "        # for dataset_fold in range(loader.num_folds()):\n",
    "        for dataset_fold in range(loader.num_folds(model.filters)):\n",
    "            print(f\"fold {dataset_fold}\", end=\" \")\n",
    "            pretrain_set, eval_set, eval_name = loader.folded_dataset(fold=dataset_fold, xtype=model.xtype, ytype=model.ytype,\n",
    "                                                                     pretrain=model.pretrainer, filters=model.filters)\n",
    "            # try:\n",
    "            if model.pretrainer:\n",
    "                print('pretrain...', end=' ')\n",
    "                model.pretrain(pretrain_set)\n",
    "                print('done', end=' ')\n",
    "\n",
    "            output = model.evaluate(eval_set)\n",
    "\n",
    "            output['architecture'] = architecture\n",
    "            output['eval_dataset'] = eval_name\n",
    "            reg_names.append(architecture)\n",
    "            reg_results.append(output)\n",
    "\n",
    "with open(CACHE_NAME, 'wb') as f:\n",
    "    pickle.dump((reg_results, reg_names), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97aae51-86f4-4243-9e8f-e7536c103933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ehd_models import EHD_Model\n",
    "\n",
    "CACHE_NAME = 'classification.cache'\n",
    "\n",
    "if NEW_CACHE:\n",
    "    class_results = []\n",
    "    class_names = []\n",
    "else:\n",
    "    with open(CACHE_NAME, 'rb') as f:\n",
    "        (class_results, class_names) = pickle.load(f)\n",
    "    \n",
    "for architecture, params in CLASS_model_architectures.items():\n",
    "    if architecture in class_names:\n",
    "        print(f'{architecture} was already evaluated - loaded from cache')\n",
    "    else:\n",
    "        print(f'\\nEvaluating classification model type: {architecture}')\n",
    "        model = EHD_Model(architecture=architecture, params=params)\n",
    "\n",
    "        for dataset_fold in range(loader.num_folds(model.filters)):\n",
    "            print(f\"fold {dataset_fold}\", end=\" \")\n",
    "            pretrain_set, eval_set, eval_name = loader.folded_dataset(fold=dataset_fold, xtype=model.xtype, ytype=model.ytype,\n",
    "                                                                     pretrain=model.pretrainer, filters=model.filters)\n",
    "            model.pretrain(pretrain_set)\n",
    "            output = model.evaluate(eval_set)\n",
    "\n",
    "            output['architecture'] = architecture\n",
    "            output['eval_dataset'] = eval_name\n",
    "            class_names.append(architecture)\n",
    "            class_results.append(output)\n",
    "\n",
    "with open(CACHE_NAME, 'wb') as f:\n",
    "    pickle.dump((class_results, class_names), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321b309-62d2-4949-80b3-e819ea6e1188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "OUT_XLSX = 'eval_summary.xlsx'\n",
    "xlsx_writer = pd.ExcelWriter(OUT_XLSX)\n",
    "plt.rcParams['figure.figsize'] = (11.0, 10.0) # set default size of plots\n",
    "plt.clf()\n",
    "\n",
    "def n_walk_result_vis(df, x, y, trends, ax=None, outfile=None, legend='auto', labels=None):\n",
    "    bar_ci = 0.95  # 'sd' for standard deviation\n",
    "    sns.lineplot(\n",
    "        data=df.loc[~df['unique N']],\n",
    "        x=x, y=y, hue=trends, style=trends,\n",
    "        markers=True,\n",
    "        err_style=\"bars\", ci=bar_ci,\n",
    "        markersize=7,\n",
    "        linewidth=1.8,\n",
    "        ax=ax,\n",
    "        legend=legend\n",
    "    )\n",
    "    ticks = df[x].loc[~df['unique N']].unique()\n",
    "    ax.set_xticks(ticks=ticks,\n",
    "               labels=np.round(10**ticks).astype(int))\n",
    "    ax.set_xlabel('dataset size')\n",
    "    if legend is not None and labels is not None:\n",
    "        ax.legend(labels)\n",
    "    if y == 'MSE':\n",
    "        ax.set_ylim(0, 6e9)\n",
    "    if y == 'MAE':\n",
    "        ax.set_ylim(0, 60_000)\n",
    "    if y == 'r':\n",
    "        ax.set_ylim(0, 1)\n",
    "    if y == 'F1':\n",
    "        ax.set_ylim(0.49, 1)\n",
    "    if y == 'AUC':\n",
    "        ax.set_ylim(0.49, 1)\n",
    "    if y == 'MAPE':\n",
    "        ax.set_ylim(0, 100)\n",
    "    if outfile is not None:\n",
    "        plt.savefig(outfile, bbox_inches='tight', pad_inches=0.1, transparent=False, dpi=500)\n",
    "        plt.clf()\n",
    "\n",
    "# Expand, log, and vis regression results\n",
    "df = pd.concat(reg_results, ignore_index=True)\n",
    "df['log_size'] = df['train_size'].apply(lambda x: np.log10(x).round(3))\n",
    "df['MAPE'] = df['MAPE'].apply(lambda x: 100 * x)\n",
    "df['unique N'] = df['train_size'].apply(lambda x: (df['train_size'] == x).sum() <= len(df['architecture'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "legend = 'auto'\n",
    "labels = ('MLE', 'MLP: last X/Y', 'Rand. Forest: pretrain only', 'Rand. Forest: last X/Y', 'Rand. Forest: cold start')\n",
    "for i, metric in enumerate(['MAE', 'r']):  # MSLE and MAPE left out for now + 'MSE', \n",
    "    #n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', outfile=f\"{metric}.png\")\n",
    "    n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', ax=ax[0][i], legend=legend, labels=labels)\n",
    "    labels = None\n",
    "    legend = False\n",
    "    \n",
    "df.to_excel(xlsx_writer, index=False, sheet_name='Regression')\n",
    "\n",
    "# Expand, log, and vis classification results\n",
    "df = pd.concat(class_results, ignore_index=True)\n",
    "df['log_size'] = df['train_size'].apply(lambda x: np.log10(x).round(3))\n",
    "df['unique N'] = df['train_size'].apply(lambda x: (df['train_size'] == x).sum() <= len(df['architecture'].unique()))\n",
    "    \n",
    "df.to_excel(xlsx_writer, index=False, sheet_name='Classification')\n",
    "\n",
    "legend = 'auto'\n",
    "labels = ('MLE', 'MLP: last X/Y', 'Rand. Forest: pretrain only', 'Rand. Forest: last X/Y', 'Rand. Forest: cold start', 'MLP: cold start')\n",
    "for i, metric in enumerate(['AUC', 'F1']):\n",
    "    #n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', outfile=f\"{metric}.png\")\n",
    "    n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', ax=ax[1][i], legend=legend, labels=labels)\n",
    "    labels = None\n",
    "    legend = False\n",
    "\n",
    "\n",
    "plt.savefig('4-square_metrics.png', bbox_inches='tight', pad_inches=0.1, transparent=False, dpi=300)\n",
    "plt.clf()\n",
    "xlsx_writer.save()\n",
    "xlsx_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
