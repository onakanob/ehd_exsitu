{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971e0bed-b911-4643-8d7d-0bc136550c44",
   "metadata": {},
   "source": [
    "# Exploring multiple approaches to machine learning across multiple small EHD experimental datasets\n",
    " - The task is to predict (regress) the size of printed features, as a function of waveform inputs to the EHD printer\n",
    " - A second task is to predict (classify) whether a given waveform input will produce any printed pattern at all. (This is equivalent to understanding the print onset voltage threshold.)\n",
    " - Multiple hidden confounding variables are likely, such as ink/tip/substrate/atmospheric condition; ink and tip clogging; print tip height from the substrate. Some of these will be relatively constant for each run of experiments, some will vary from feature to feature.\n",
    " - Ink dynamics at the printing tip are complex and nonlinear, with electrical and fluid/acoustic phenomena that interact with each other.\n",
    "\n",
    "## Goals\n",
    " - In regression, aim for <3% mean absolute error (from the predicted outcome)\n",
    " - In classification, aim for >0.9 ROC AUC\n",
    " - For a new/test set, achieve these in <=100 experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30913236-8a5d-4983-b31b-9ae56f38dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "sys.path.append('..')\n",
    "from ehd_dataset import EHD_Loader\n",
    "from ehd_models import EHD_Model\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c326ac0-8e77-4024-8fde-d3bfe5aaa08c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load 10-Mar-2022 large nozzle mosaic: 'DataFrame' object has no attribute 'note'\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\29-Mar-2022 lg 1cm 300 points\t263 points\toffset 2\tcorr 0.4979414348873561\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\2-May-2022__run 1\t121 points\toffset 32\tcorr 0.6417618745477631\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\2-May-2022__run 2\t528 points\toffset 4\tcorr 0.6184015138069094\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\23-May-2022_squares\t352 points\toffset 8\tcorr 0.5521885646579606\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\24-May-2022 large harmonics\t682 points\toffset 0\tcorr 0.5871179189117662\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\8-Aug-2022_lg-square-20um\t686 points\toffset 1\tcorr 0.8454268563076222\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\10-Sep-2022_std-square-10um\t149 points\toffset 16\tcorr 0.7935638727333442\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\10-Sep-2022_std-square-20um\t643 points\toffset 134\tcorr 0.8161189702410045\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\13-Sep-2022_std-square-30um\t458 points\toffset 4\tcorr 0.7701116025503417\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\13-Sep-2022_std-square-40um\t250 points\toffset 6\tcorr 0.793067116772568\n",
      "Failed to load 22-Sep-2022_sfine-square-10um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\22-Sep-2022_sfine-square-10um\\\\measurements.xlsx'\n",
      "Failed to load 22-Sep-2022_sfine-square-20um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\22-Sep-2022_sfine-square-20um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 27-Sep-22_std-sines-10um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\27-Sep-22_std-sines-10um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 27-Sep-22_std-sines-20um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\27-Sep-22_std-sines-20um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 27-Sep-22_std-sines-30um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\27-Sep-22_std-sines-30um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 11-Oct-22_sfine-sines-10um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\11-Oct-22_sfine-sines-10um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load nan: join() argument must be str, bytes, or os.PathLike object, not 'float'\n",
      "Failed to load nan: join() argument must be str, bytes, or os.PathLike object, not 'float'\n",
      "Datasets Loaded!\n",
      ">> Quick correlation validation check -- [auac; vec L2] <<\n",
      "<<29-Mar-2022 lg 1cm 300 points -- [0.498; 0.661]>> <<2-May-2022__run 1 -- [0.642; 0.826]>> <<2-May-2022__run 2 -- [0.618; 0.796]>> <<23-May-2022_squares -- [0.552; 0.658]>> <<24-May-2022 large harmonics -- [0.587; 0.763]>> <<8-Aug-2022_lg-square-20um -- [0.845; 0.532]>> <<10-Sep-2022_std-square-10um -- [0.794; 0.654]>> <<10-Sep-2022_std-square-20um -- [0.816; 0.870]>> <<13-Sep-2022_std-square-30um -- [0.770; 0.739]>> <<13-Sep-2022_std-square-40um -- [0.793; 0.861]>> "
     ]
    }
   ],
   "source": [
    "INDEX = \"C:/Dropbox/SPEED/Self Driving EHD/Datasets/dataset_index.xlsx\"\n",
    "# INDEX = \"C:/Users/Oliver/Dropbox/SPEED/Self Driving EHD/Datasets/dataset_index.xlsx\"\n",
    "loader = EHD_Loader(INDEX)\n",
    "print(\"Datasets Loaded!\\n>> Quick correlation validation check -- [auac; vec L2] <<\")\n",
    "\n",
    "for i, df in enumerate(loader.datasets):\n",
    "    AUAC, _ = pearsonr(df.area,\n",
    "                       df.wave.apply(lambda x: np.sum(np.abs(x))))\n",
    "    VL2, _ = pearsonr(df.area,\n",
    "                       df.vector.apply(lambda x: np.sqrt(np.sum(x**2))))\n",
    "    print(f\"<<{loader.names[i]} -- [{AUAC:.3f}; {VL2:.3f}]\", end='>> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977c73f2-3d10-45e6-b199-6930533ef4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = loader.datasets[0]\n",
    "np.isnan(df['clogging'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5afbe9d-9041-40c6-9389-633bf5cb5eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting previous results from MLE\n",
      "\n",
      "Evaluating regression model type: MLE\n",
      "fold 0 pretrain... done fold 1 pretrain... done fold 2 pretrain... done fold 3 pretrain... done Overwriting previous results from cold_RF\n",
      "\n",
      "Evaluating regression model type: cold_RF\n",
      "fold 0 fold 1 fold 2 fold 3 Overwriting previous results from only_pretrained_RF\n",
      "\n",
      "Evaluating regression model type: only_pretrained_RF\n",
      "fold 0 pretrain... done fold 1 pretrain... done fold 2 pretrain... done fold 3 pretrain... done "
     ]
    }
   ],
   "source": [
    "# REGRESSION MODELS\n",
    "from ehd_models import EHD_Model\n",
    "\n",
    "# Print Area Regression Models <<<<<<<<<<<<<<<<<<<<\n",
    "REGRESSION_model_architectures = {'MLE': {},\n",
    "                                  'cold_RF': {},\n",
    "                                  'only_pretrained_RF': {},\n",
    "                                  # 'cold_MLP': {'max_iter': 100_000},\n",
    "                                  # 'lastXY_MLP': {'max_iter': 200_000},\n",
    "                                  # 'lastXY_RF': {}\n",
    "                                 }\n",
    "NEW_CACHE = False\n",
    "OVERWRITE_RESULTS = True\n",
    "\n",
    "XTYPE = \"vector\"\n",
    "YTYPE = \"max_width\"\n",
    "FILTERS = [\n",
    "           ('vector',  lambda x: len(x), 2),\n",
    "           ('Wavegen', lambda x: x, 'square'),\n",
    "           ('V Thresh [V] @ .5s', np.isnan, False),\n",
    "           ('jetted',  lambda x: x, True),\n",
    "           ('clogging', lambda x: x, False)\n",
    "          ]\n",
    "\n",
    "CACHE_NAME = 'regression.cache'\n",
    "\n",
    "if NEW_CACHE:\n",
    "    reg_results = []\n",
    "    reg_names = []\n",
    "else:\n",
    "    with open(CACHE_NAME, 'rb') as f:\n",
    "        (reg_results, reg_names) = pickle.load(f)\n",
    "\n",
    "for architecture, params in REGRESSION_model_architectures.items():\n",
    "    if (architecture in reg_names) and not OVERWRITE_RESULTS:\n",
    "        print(f'{architecture} was already evaluated - loaded from cache')\n",
    "    else:\n",
    "        print('')\n",
    "        if architecture in reg_names:\n",
    "            print(f\"Overwriting previous results from {architecture}\")\n",
    "            reg_results = [reg_results[i] for i in range(len(reg_results)) if reg_names[i] != architecture]\n",
    "            reg_names = [r for r in reg_names if r != architecture]\n",
    "        print(f'Evaluating regression model type: {architecture}')\n",
    "        model = EHD_Model(architecture=architecture, params=params)\n",
    "\n",
    "        # for dataset_fold in range(loader.num_folds()):\n",
    "        for dataset_fold in range(loader.num_folds(FILTERS)):\n",
    "            print(f\"fold {dataset_fold}\", end=\" \")\n",
    "            pretrain_set, eval_set, eval_name = loader.folded_dataset(fold=dataset_fold, xtype=XTYPE, ytype=YTYPE,\n",
    "                                                                     pretrain=model.pretrainer, filters=FILTERS)\n",
    "            # try:\n",
    "            if model.pretrainer:\n",
    "                print('pretrain...', end=' ')\n",
    "                model.pretrain(pretrain_set)\n",
    "                print('done', end=' ')\n",
    "\n",
    "            output = model.evaluate(eval_set)\n",
    "\n",
    "            output['architecture'] = architecture\n",
    "            output['eval_dataset'] = eval_name\n",
    "            reg_names.append(architecture)\n",
    "            reg_results.append(output)\n",
    "\n",
    "with open(CACHE_NAME, 'wb') as f:\n",
    "    pickle.dump((reg_results, reg_names), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97aae51-86f4-4243-9e8f-e7536c103933",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting previous results from MLE_class\n",
      "\n",
      "Evaluating classification model type: MLE_class\n",
      "fold 0 fold 1 fold 2 fold 3 Overwriting previous results from cold_RF_class\n",
      "\n",
      "Evaluating classification model type: cold_RF_class\n",
      "fold 0 fold 1 fold 2 fold 3 Overwriting previous results from only_pretrained_RF_class\n",
      "\n",
      "Evaluating classification model type: only_pretrained_RF_class\n",
      "fold 0 fold 1 fold 2 fold 3 "
     ]
    }
   ],
   "source": [
    "# JETTING CLASSIFICATION MODELS\n",
    "from ehd_models import EHD_Model\n",
    "\n",
    "# Jetting Classification Models <<<<<<<<<<<<<<<<<<<<\n",
    "CLASS_model_architectures = {'MLE_class': {},\n",
    "                             'cold_RF_class': {},\n",
    "                             'only_pretrained_RF_class': {},\n",
    "                             # 'cold_MLP_class': {'max_iter': 100_000},\n",
    "                             # 'lastXY_MLP_class': {'max_iter': 200_000},\n",
    "                             # 'lastXY_RF_class': {}\n",
    "                            }\n",
    "NEW_CACHE = False\n",
    "OVERWRITE_RESULTS = True\n",
    "\n",
    "XTYPE = \"vector\"\n",
    "YTYPE = \"jetted\"\n",
    "FILTERS = [\n",
    "           ('vector', lambda x: len(x), 2),\n",
    "           ('Wavegen', lambda x: x, 'square'),\n",
    "           ('V Thresh [V] @ .5s', np.isnan, False),\n",
    "           ('clogging', lambda x: x, False)\n",
    "          ]\n",
    "\n",
    "CACHE_NAME = 'classification.cache'\n",
    "\n",
    "if NEW_CACHE:\n",
    "    class_results = []\n",
    "    class_names = []\n",
    "else:\n",
    "    with open(CACHE_NAME, 'rb') as f:\n",
    "        (class_results, class_names) = pickle.load(f)\n",
    "    \n",
    "for architecture, params in CLASS_model_architectures.items():\n",
    "    if (architecture in class_names) and not OVERWRITE_RESULTS:\n",
    "        print(f'{architecture} was already evaluated - loaded from cache')\n",
    "    else:\n",
    "        if architecture in class_names:\n",
    "            print(f\"Overwriting previous results from {architecture}\")\n",
    "            class_results = [class_results[i] for i in range(len(class_results)) if class_names[i] != architecture]\n",
    "            class_names = [c for c in class_names if c != architecture]\n",
    "        print(f'\\nEvaluating classification model type: {architecture}')\n",
    "        model = EHD_Model(architecture=architecture, params=params)\n",
    "\n",
    "        for dataset_fold in range(loader.num_folds(FILTERS)):\n",
    "            print(f\"fold {dataset_fold}\", end=\" \")\n",
    "            pretrain_set, eval_set, eval_name = loader.folded_dataset(fold=dataset_fold, xtype=XTYPE, ytype=YTYPE,\n",
    "                                                                     pretrain=model.pretrainer, filters=FILTERS)\n",
    "            model.pretrain(pretrain_set)\n",
    "            output = model.evaluate(eval_set)\n",
    "\n",
    "            output['architecture'] = architecture\n",
    "            output['eval_dataset'] = eval_name\n",
    "            class_names.append(architecture)\n",
    "            class_results.append(output)\n",
    "\n",
    "with open(CACHE_NAME, 'wb') as f:\n",
    "    pickle.dump((class_results, class_names), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4321b309-62d2-4949-80b3-e819ea6e1188",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 792x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE FIG and SUMMARY FILES\n",
    "import seaborn as sns\n",
    "\n",
    "OUT_XLSX = 'eval_summary.xlsx'\n",
    "xlsx_writer = pd.ExcelWriter(OUT_XLSX)\n",
    "plt.rcParams['figure.figsize'] = (11.0, 10.0) # set default size of plots\n",
    "plt.clf()\n",
    "\n",
    "def n_walk_result_vis(df, x, y, trends, ax=None, outfile=None, legend='auto', labels=None):\n",
    "    bar_ci = 0.95  # 'sd' for standard deviation\n",
    "    sns.lineplot(\n",
    "        data=df.loc[~df['unique N']],\n",
    "        x=x, y=y, hue=trends, style=trends,\n",
    "        markers=True,\n",
    "        err_style=\"bars\", ci=bar_ci,\n",
    "        markersize=7,\n",
    "        linewidth=1.8,\n",
    "        ax=ax,\n",
    "        legend=legend\n",
    "    )\n",
    "    ticks = df[x].loc[~df['unique N']].unique()\n",
    "    ax.set_xticks(ticks=ticks,\n",
    "               labels=np.round(10**ticks).astype(int))\n",
    "    ax.set_xlabel('dataset size')\n",
    "    if legend is not None and labels is not None:\n",
    "        ax.legend(labels)\n",
    "    if y == 'MSE':\n",
    "        ax.set_ylim(0, 16e3)\n",
    "    if y == 'MAE':\n",
    "        ax.set_ylim(0, 40)\n",
    "    if y == 'r':\n",
    "        ax.set_ylim(0, 1)\n",
    "    if y == 'F1':\n",
    "        ax.set_ylim(0.49, 1)\n",
    "    if y == 'AUC':\n",
    "        ax.set_ylim(0.49, 1)\n",
    "    if y == 'MAPE':\n",
    "        ax.set_ylim(0, 100)\n",
    "    if outfile is not None:\n",
    "        plt.savefig(outfile, bbox_inches='tight', pad_inches=0.1, transparent=False, dpi=500)\n",
    "        plt.clf()\n",
    "\n",
    "# REGRESSION: Expand, log, and visualize results\n",
    "df = pd.concat(reg_results, ignore_index=True)\n",
    "df['log_size'] = df['train_size'].apply(lambda x: np.log10(x).round(3))\n",
    "df['MAPE'] = df['MAPE'].apply(lambda x: 100 * x)\n",
    "df['unique N'] = df['train_size'].apply(lambda x: (df['train_size'] == x).sum() <= len(df['architecture'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "legend = 'auto'\n",
    "# labels = ('MLE', 'MLP: last X/Y', 'Rand. Forest: pretrain only', 'Rand. Forest: last X/Y', 'Rand. Forest: cold start')\n",
    "labels = ('MLE', 'Cold Random Forest', 'Pretrained Random Forest')\n",
    "for i, metric in enumerate(['MAE', 'r']):  # MSLE and MAPE left out for now + 'MSE', \n",
    "    #n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', outfile=f\"{metric}.png\")\n",
    "    n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', ax=ax[0][i], legend=legend, labels=labels)\n",
    "    labels = None\n",
    "    legend = False\n",
    "    \n",
    "df.to_excel(xlsx_writer, index=False, sheet_name='Regression')\n",
    "\n",
    "# CLASSIFICATION: Expand, log, and visualize results\n",
    "df = pd.concat(class_results, ignore_index=True)\n",
    "df['log_size'] = df['train_size'].apply(lambda x: np.log10(x).round(3))\n",
    "df['unique N'] = df['train_size'].apply(lambda x: (df['train_size'] == x).sum() <= len(df['architecture'].unique()))\n",
    "    \n",
    "df.to_excel(xlsx_writer, index=False, sheet_name='Classification')\n",
    "\n",
    "legend = 'auto'\n",
    "labels = ('MLE', 'Cold Random Forest', 'Pretrained Random Forest')\n",
    "# labels = ('MLE', 'MLP: last X/Y', 'Rand. Forest: pretrain only', 'Rand. Forest: last X/Y', 'Rand. Forest: cold start', 'MLP: cold start')\n",
    "for i, metric in enumerate(['AUC', 'F1']):\n",
    "    #n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', outfile=f\"{metric}.png\")\n",
    "    n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', ax=ax[1][i], legend=legend, labels=labels)\n",
    "    labels = None\n",
    "    legend = False\n",
    "\n",
    "\n",
    "plt.savefig('4-square_metrics.png', bbox_inches='tight', pad_inches=0.1, transparent=False, dpi=300)\n",
    "plt.clf()\n",
    "xlsx_writer.save()\n",
    "xlsx_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
