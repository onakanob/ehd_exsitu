{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971e0bed-b911-4643-8d7d-0bc136550c44",
   "metadata": {},
   "source": [
    "# Exploring multiple approaches to machine learning across multiple small EHD experimental datasets\n",
    " - The task is to predict (regress) the size of printed features, as a function of waveform inputs to the EHD printer\n",
    " - A second task is to predict (classify) whether a given waveform input will produce any printed pattern at all. (This is equivalent to understanding the print onset voltage threshold.)\n",
    " - Multiple hidden confounding variables are likely, such as ink/tip/substrate/atmospheric condition; ink and tip clogging; print tip height from the substrate. Some of these will be relatively constant for each run of experiments, some will vary from feature to feature.\n",
    " - Ink dynamics at the printing tip are complex and nonlinear, with electrical and fluid/acoustic phenomena that interact with each other.\n",
    "\n",
    "## Goals\n",
    " - In regression, aim for <3% mean absolute error (from the predicted outcome)\n",
    " - In classification, aim for >0.9 ROC AUC\n",
    " - For a new/test set, achieve these in <=100 experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30913236-8a5d-4983-b31b-9ae56f38dbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "sys.path.append('..')\n",
    "from ehd_dataset import EHD_Loader\n",
    "from ehd_models import EHD_Model\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "NEW_CACHE = True\n",
    "# TODO option to bypass the cache op for nondestructive testing\n",
    "\n",
    "# Print Area Regression Models <<<<<<<<<<<<<<<<<<<<\n",
    "REGRESSION_model_architectures = {'MLE': {},\n",
    "                                  'cold_RF': {},\n",
    "                                  'only_pretrained_RF': {}}\n",
    "                                  # 'cold_MLP': {'max_iter': 100_000},\n",
    "                                  # 'lastXY_MLP': {'max_iter': 200_000},\n",
    "                                  # 'lastXY_RF': {}}\n",
    "\n",
    "# Jetting Classification Models <<<<<<<<<<<<<<<<<<<<\n",
    "CLASS_model_architectures = {'MLE_class': {},\n",
    "                             'cold_RF_class': {},\n",
    "                             'only_pretrained_RF_class': {}}\n",
    "                             # 'cold_MLP_class': {'max_iter': 100_000},\n",
    "                             # 'lastXY_MLP_class': {'max_iter': 200_000},\n",
    "                             # 'lastXY_RF_class': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c326ac0-8e77-4024-8fde-d3bfe5aaa08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load 10-Mar-2022 large nozzle mosaic: 'DataFrame' object has no attribute 'note'\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\29-Mar-2022 lg 1cm 300 points\t263 points\toffset 2\tcorr 0.4979414348873561\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\2-May-2022__run 1\t121 points\toffset 32\tcorr 0.6417618745477631\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\2-May-2022__run 2\t528 points\toffset 4\tcorr 0.6184015138069094\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\23-May-2022_squares\t352 points\toffset 8\tcorr 0.5521885646579606\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\24-May-2022 large harmonics\t682 points\toffset 0\tcorr 0.5871179189117662\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\8-Aug-2022_lg-square-20um\t686 points\toffset 1\tcorr 0.8454268563076222\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\10-Sep-2022_std-square-10um\t149 points\toffset 16\tcorr 0.7935638727333442\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\10-Sep-2022_std-square-20um\t643 points\toffset 134\tcorr 0.8161189702410045\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\13-Sep-2022_std-square-30um\t458 points\toffset 4\tcorr 0.7701116025503417\n",
      "dataset C:\\Dropbox\\SPEED\\Self Driving EHD\\Datasets\\13-Sep-2022_std-square-40um\t250 points\toffset 6\tcorr 0.793067116772568\n",
      "Failed to load 22-Sep-2022_sfine-square-10um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\22-Sep-2022_sfine-square-10um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 22-Sep-2022_sfine-square-20um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\22-Sep-2022_sfine-square-20um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 27-Sep-22_std-sines-10um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\27-Sep-22_std-sines-10um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 27-Sep-22_std-sines-20um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\27-Sep-22_std-sines-20um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 27-Sep-22_std-sines-30um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\27-Sep-22_std-sines-30um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load 11-Oct-22_sfine-sines-10um: [Errno 2] No such file or directory: 'C:\\\\Dropbox\\\\SPEED\\\\Self Driving EHD\\\\Datasets\\\\11-Oct-22_sfine-sines-10um\\\\results 2-may-22 run2.xlsx'\n",
      "Failed to load nan: join() argument must be str, bytes, or os.PathLike object, not 'float'\n",
      "Failed to load nan: join() argument must be str, bytes, or os.PathLike object, not 'float'\n",
      "Datasets Loaded!\n",
      ">> Quick correlation validation check -- [auac; vec L2] <<\n",
      "<<29-Mar-2022 lg 1cm 300 points -- [0.498; 0.661]>> <<2-May-2022__run 1 -- [0.642; 0.826]>> <<2-May-2022__run 2 -- [0.618; 0.796]>> <<23-May-2022_squares -- [0.552; 0.658]>> <<24-May-2022 large harmonics -- [0.587; 0.763]>> <<8-Aug-2022_lg-square-20um -- [0.845; 0.532]>> <<10-Sep-2022_std-square-10um -- [0.794; 0.654]>> <<10-Sep-2022_std-square-20um -- [0.816; 0.870]>> <<13-Sep-2022_std-square-30um -- [0.770; 0.739]>> <<13-Sep-2022_std-square-40um -- [0.793; 0.861]>> "
     ]
    }
   ],
   "source": [
    "INDEX = \"C:/Dropbox/SPEED/Self Driving EHD/Datasets/dataset_index.xlsx\"\n",
    "# INDEX = \"C:/Users/Oliver/Dropbox/SPEED/Self Driving EHD/Datasets/dataset_index.xlsx\"\n",
    "loader = EHD_Loader(INDEX)\n",
    "print(\"Datasets Loaded!\\n>> Quick correlation validation check -- [auac; vec L2] <<\")\n",
    "\n",
    "for i, df in enumerate(loader.datasets):\n",
    "    AUAC, _ = pearsonr(df.area,\n",
    "                       df.wave.apply(lambda x: np.sum(np.abs(x))))\n",
    "    VL2, _ = pearsonr(df.area,\n",
    "                       df.vector.apply(lambda x: np.sqrt(np.sum(x**2))))\n",
    "    print(f\"<<{loader.names[i]} -- [{AUAC:.3f}; {VL2:.3f}]\", end='>> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5afbe9d-9041-40c6-9389-633bf5cb5eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating regression model type: MLE\n",
      "fold 0 pretrain... done fold 1 pretrain... done fold 2 pretrain... done fold 3 pretrain... done fold 4 pretrain... done \n",
      "Evaluating regression model type: cold_RF\n",
      "fold 0 fold 1 fold 2 fold 3 fold 4 \n",
      "Evaluating regression model type: only_pretrained_RF\n",
      "fold 0 pretrain... done fold 1 pretrain... done fold 2 pretrain... done fold 3 pretrain... done fold 4 pretrain... done "
     ]
    }
   ],
   "source": [
    "from ehd_models import EHD_Model\n",
    "\n",
    "XTYPE = \"vector\"\n",
    "YTYPE = \"max_width\"\n",
    "FILTERS = [('vector', lambda x: len(x), 2)]\n",
    "\n",
    "CACHE_NAME = 'regression.cache'\n",
    "\n",
    "if NEW_CACHE:\n",
    "    reg_results = []\n",
    "    reg_names = []\n",
    "else:\n",
    "    with open(CACHE_NAME, 'rb') as f:\n",
    "        (reg_results, reg_names) = pickle.load(f)\n",
    "\n",
    "for architecture, params in REGRESSION_model_architectures.items():\n",
    "    if architecture in reg_names:\n",
    "        print(f'{architecture} was already evaluated - loaded from cache')\n",
    "    else:\n",
    "        print(f'\\nEvaluating regression model type: {architecture}')\n",
    "        model = EHD_Model(architecture=architecture, params=params)\n",
    "\n",
    "        # for dataset_fold in range(loader.num_folds()):\n",
    "        for dataset_fold in range(loader.num_folds(FILTERS)):\n",
    "            print(f\"fold {dataset_fold}\", end=\" \")\n",
    "            pretrain_set, eval_set, eval_name = loader.folded_dataset(fold=dataset_fold, xtype=XTYPE, ytype=YTYPE,\n",
    "                                                                     pretrain=model.pretrainer, filters=FILTERS)\n",
    "            # try:\n",
    "            if model.pretrainer:\n",
    "                print('pretrain...', end=' ')\n",
    "                model.pretrain(pretrain_set)\n",
    "                print('done', end=' ')\n",
    "\n",
    "            output = model.evaluate(eval_set)\n",
    "\n",
    "            output['architecture'] = architecture\n",
    "            output['eval_dataset'] = eval_name\n",
    "            reg_names.append(architecture)\n",
    "            reg_results.append(output)\n",
    "\n",
    "with open(CACHE_NAME, 'wb') as f:\n",
    "    pickle.dump((reg_results, reg_names), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a97aae51-86f4-4243-9e8f-e7536c103933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating classification model type: MLE_class\n",
      "fold 0 fold 1 fold 2 fold 3 fold 4 \n",
      "Evaluating classification model type: cold_RF_class\n",
      "fold 0 "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.dtype[bool_]' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\numpy\\core\\getlimits.py:384\u001b[0m, in \u001b[0;36mfinfo.__new__\u001b[1;34m(cls, dtype)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mnumeric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;66;03m# In case a float instance was given\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.dtype[bool_]' object is not callable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m pretrain_set, eval_set, eval_name \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mfolded_dataset(fold\u001b[38;5;241m=\u001b[39mdataset_fold, xtype\u001b[38;5;241m=\u001b[39mXTYPE, ytype\u001b[38;5;241m=\u001b[39mYTYPE,\n\u001b[0;32m     26\u001b[0m                                                          pretrain\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpretrainer, filters\u001b[38;5;241m=\u001b[39mFILTERS)\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mpretrain(pretrain_set)\n\u001b[1;32m---> 28\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m architecture\n\u001b[0;32m     31\u001b[0m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m eval_name\n",
      "File \u001b[1;32mC:\\Dropbox\\Python\\ehd_exsitu\\notebooks\\..\\ehd_models\\Model.py:95\u001b[0m, in \u001b[0;36mEHD_Model.evaluate\u001b[1;34m(self, dataset, train_sizes, test_size, samples_to_try)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# retrain self.model on the train set\u001b[39;00m\n\u001b[0;32m     94\u001b[0m temp_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 95\u001b[0m \u001b[43mtemp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m result \u001b[38;5;241m=\u001b[39m temp_model\u001b[38;5;241m.\u001b[39mevaluate(test_set)\n\u001b[0;32m     97\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_size\n",
      "File \u001b[1;32mC:\\Dropbox\\Python\\ehd_exsitu\\notebooks\\..\\ehd_models\\cold_models.py:30\u001b[0m, in \u001b[0;36mCold_SciKit_Model.retrain\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\sklearn\\pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    389\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 390\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\sklearn\\pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    346\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    349\u001b[0m     cloned_transformer,\n\u001b[0;32m    350\u001b[0m     X,\n\u001b[0;32m    351\u001b[0m     y,\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    353\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    354\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    356\u001b[0m )\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\sklearn\\base.py:855\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:806\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:944\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;66;03m# Extract the list of near constant features on the raw variances,\u001b[39;00m\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;66;03m# before taking the square root.\u001b[39;00m\n\u001b[1;32m--> 944\u001b[0m     constant_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_is_constant_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples_seen_\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_ \u001b[38;5;241m=\u001b[39m _handle_zeros_in_scale(\n\u001b[0;32m    948\u001b[0m         np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, constant_mask\u001b[38;5;241m=\u001b[39mconstant_mask\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:78\u001b[0m, in \u001b[0;36m_is_constant_feature\u001b[1;34m(var, mean, n_samples)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"Detect if a feature is indistinguishable from a constant feature.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03mThe detection is based on its computed variance and on the theoretical\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03mrecommendations\", by Chan, Golub, and LeVeque.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# In scikit-learn, variance is always computed using float64 accumulators.\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meps\n\u001b[0;32m     80\u001b[0m upper_bound \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;241m*\u001b[39m eps \u001b[38;5;241m*\u001b[39m var \u001b[38;5;241m+\u001b[39m (n_samples \u001b[38;5;241m*\u001b[39m mean \u001b[38;5;241m*\u001b[39m eps) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m var \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m upper_bound\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\ehd\\lib\\site-packages\\numpy\\core\\getlimits.py:387\u001b[0m, in \u001b[0;36mfinfo.__new__\u001b[1;34m(cls, dtype)\u001b[0m\n\u001b[0;32m    384\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m numeric\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;66;03m# In case a float instance was given\u001b[39;00m\n\u001b[1;32m--> 387\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[43mnumeric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_finfo_cache\u001b[38;5;241m.\u001b[39mget(dtype, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.dtype[bool_]' object is not callable"
     ]
    }
   ],
   "source": [
    "from ehd_models import EHD_Model\n",
    "\n",
    "XTYPE = \"vector\"\n",
    "YTYPE = \"jetted\"\n",
    "FILTERS = [('vector', lambda x: len(x), 2)]\n",
    "\n",
    "CACHE_NAME = 'classification.cache'\n",
    "\n",
    "if NEW_CACHE:\n",
    "    class_results = []\n",
    "    class_names = []\n",
    "else:\n",
    "    with open(CACHE_NAME, 'rb') as f:\n",
    "        (class_results, class_names) = pickle.load(f)\n",
    "    \n",
    "for architecture, params in CLASS_model_architectures.items():\n",
    "    if architecture in class_names:\n",
    "        print(f'{architecture} was already evaluated - loaded from cache')\n",
    "    else:\n",
    "        print(f'\\nEvaluating classification model type: {architecture}')\n",
    "        model = EHD_Model(architecture=architecture, params=params)\n",
    "\n",
    "        for dataset_fold in range(loader.num_folds(FILTERS)):\n",
    "            print(f\"fold {dataset_fold}\", end=\" \")\n",
    "            pretrain_set, eval_set, eval_name = loader.folded_dataset(fold=dataset_fold, xtype=XTYPE, ytype=YTYPE,\n",
    "                                                                     pretrain=model.pretrainer, filters=FILTERS)\n",
    "            model.pretrain(pretrain_set)\n",
    "            output = model.evaluate(eval_set)\n",
    "\n",
    "            output['architecture'] = architecture\n",
    "            output['eval_dataset'] = eval_name\n",
    "            class_names.append(architecture)\n",
    "            class_results.append(output)\n",
    "\n",
    "with open(CACHE_NAME, 'wb') as f:\n",
    "    pickle.dump((class_results, class_names), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321b309-62d2-4949-80b3-e819ea6e1188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "OUT_XLSX = 'eval_summary.xlsx'\n",
    "xlsx_writer = pd.ExcelWriter(OUT_XLSX)\n",
    "plt.rcParams['figure.figsize'] = (11.0, 10.0) # set default size of plots\n",
    "plt.clf()\n",
    "\n",
    "def n_walk_result_vis(df, x, y, trends, ax=None, outfile=None, legend='auto', labels=None):\n",
    "    bar_ci = 0.95  # 'sd' for standard deviation\n",
    "    sns.lineplot(\n",
    "        data=df.loc[~df['unique N']],\n",
    "        x=x, y=y, hue=trends, style=trends,\n",
    "        markers=True,\n",
    "        err_style=\"bars\", ci=bar_ci,\n",
    "        markersize=7,\n",
    "        linewidth=1.8,\n",
    "        ax=ax,\n",
    "        legend=legend\n",
    "    )\n",
    "    ticks = df[x].loc[~df['unique N']].unique()\n",
    "    ax.set_xticks(ticks=ticks,\n",
    "               labels=np.round(10**ticks).astype(int))\n",
    "    ax.set_xlabel('dataset size')\n",
    "    if legend is not None and labels is not None:\n",
    "        ax.legend(labels)\n",
    "    if y == 'MSE':\n",
    "        ax.set_ylim(0, 6e9)\n",
    "    if y == 'MAE':\n",
    "        ax.set_ylim(0, 60_000)\n",
    "    if y == 'r':\n",
    "        ax.set_ylim(0, 1)\n",
    "    if y == 'F1':\n",
    "        ax.set_ylim(0.49, 1)\n",
    "    if y == 'AUC':\n",
    "        ax.set_ylim(0.49, 1)\n",
    "    if y == 'MAPE':\n",
    "        ax.set_ylim(0, 100)\n",
    "    if outfile is not None:\n",
    "        plt.savefig(outfile, bbox_inches='tight', pad_inches=0.1, transparent=False, dpi=500)\n",
    "        plt.clf()\n",
    "\n",
    "# Expand, log, and vis regression results\n",
    "df = pd.concat(reg_results, ignore_index=True)\n",
    "df['log_size'] = df['train_size'].apply(lambda x: np.log10(x).round(3))\n",
    "df['MAPE'] = df['MAPE'].apply(lambda x: 100 * x)\n",
    "df['unique N'] = df['train_size'].apply(lambda x: (df['train_size'] == x).sum() <= len(df['architecture'].unique()))\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "legend = 'auto'\n",
    "labels = ('MLE', 'MLP: last X/Y', 'Rand. Forest: pretrain only', 'Rand. Forest: last X/Y', 'Rand. Forest: cold start')\n",
    "for i, metric in enumerate(['MAE', 'r']):  # MSLE and MAPE left out for now + 'MSE', \n",
    "    #n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', outfile=f\"{metric}.png\")\n",
    "    n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', ax=ax[0][i], legend=legend, labels=labels)\n",
    "    labels = None\n",
    "    legend = False\n",
    "    \n",
    "df.to_excel(xlsx_writer, index=False, sheet_name='Regression')\n",
    "\n",
    "# Expand, log, and vis classification results\n",
    "df = pd.concat(class_results, ignore_index=True)\n",
    "df['log_size'] = df['train_size'].apply(lambda x: np.log10(x).round(3))\n",
    "df['unique N'] = df['train_size'].apply(lambda x: (df['train_size'] == x).sum() <= len(df['architecture'].unique()))\n",
    "    \n",
    "df.to_excel(xlsx_writer, index=False, sheet_name='Classification')\n",
    "\n",
    "legend = 'auto'\n",
    "labels = ('MLE', 'MLP: last X/Y', 'Rand. Forest: pretrain only', 'Rand. Forest: last X/Y', 'Rand. Forest: cold start', 'MLP: cold start')\n",
    "for i, metric in enumerate(['AUC', 'F1']):\n",
    "    #n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', outfile=f\"{metric}.png\")\n",
    "    n_walk_result_vis(df, x='log_size', y=metric, trends='architecture', ax=ax[1][i], legend=legend, labels=labels)\n",
    "    labels = None\n",
    "    legend = False\n",
    "\n",
    "\n",
    "plt.savefig('4-square_metrics.png', bbox_inches='tight', pad_inches=0.1, transparent=False, dpi=300)\n",
    "plt.clf()\n",
    "xlsx_writer.save()\n",
    "xlsx_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
